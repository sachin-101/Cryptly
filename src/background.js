import 'babel-polyfill';
import * as tf from '@tensorflow/tfjs';
import { Pipeline } from './pipeline';
import { Syft } from '@openmined/syft.js';

const FIVE_SECONDS_IN_MS = 5000;
const NEGATIVE = 0;
const NEUTRAL = 1;
const POSITIVE = 2;

/**
 * Check if blocked_urls exist in storage, if not
 * then initialize with an empty array.
 */
chrome.storage.sync.get(['blocked_urls'],data => {
    if(data.blocked_urls === undefined) {        
        chrome.storage.sync.set({'blocked_urls': []}, () => {
            console.log("Initialized blocked urls array");
        });
    }
});

/**
 * Add listener to recieve requests from tabs, and pass the text to 
 * pipeline and then to model for inference. Returns a response
 * to the tab with the prediction.
 * 
 * @param: message: { 
 *              action: Type of inference task,
 *              textAreaId: ID of the text Area on the page,
 *              text: Text which needs to be analyzed
 *          }
*/
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
    console.log('Recieved request', request);
    if (request && request.action && request.textAreaId && request.text) {
        
        if (request.action ==  'TEXT_SENTIMENT') {
        
            pipeline.process(request.text)
            .then(indices => sentimentClassifier.analyzeText(indices))
            .then(prediction => {
                chrome.tabs.sendMessage(sender.tab.id, {
                    action: 'TEXT_SENTIMENT_CLASSIFIED',
                    prediction: {'POSITIVE': prediction, 'NEGATIVE': 1 - prediction},
                    textAreaId: request.textAreaId
                });
            })
            .catch(err => console.log(err)); 
        }      
    }
});

const ROOT_URL = 'http://localhost:5000/imdb_review/'
const MODEL_URL = ROOT_URL + 'model.json'
const WORD2INDEX_URL = ROOT_URL + 'word2index.json'
const META_DATA_URL = ROOT_URL + 'meta.json'

class SentimentClassifier {
    constructor() {
        this.loadModel();
    }

    /**
     * Loads sentimentClassifier from URL and keeps a reference to it in the object.
     */
    async loadModel() {
        console.log('Loading model...');
        try {
            this.model = await tf.loadLayersModel(MODEL_URL);
            console.log("Model loaded.")
        } catch (e){
            console.error(`Unable to load model from URL: ${MODEL_URL}`);
            console.error(e);
        }
    }

    
    /**
     * Triggers the model to make a prediction on the text provided.
     *
     * @param {string} indices the indices output by Pipeline
     * @param {string} textAreaId Id of the textArea which has text
     * @param {*} tabId tab Id   
     */
    async analyzeText(indices) {
        
        if (!this.model) {
            console.log('Waiting for model to load....');
            setTimeout(() => { this.analyzeText(text) }, FIVE_SECONDS_IN_MS); // try after 5 second
            return;
        }

        try {

            const inputTensor = tf.tensor2d(indices, [1, indices.length], 'int32');
            inputTensor.print();

            // returns a tensor which is converte to array asynchronously
            let prediction = await this.model.predict(inputTensor).data();
            console.log("value ",prediction[0]);
            return prediction[0];
            
        } catch (err) {
            console.log("Error", err);
        }
    }
}

const sentimentClassifier = new SentimentClassifier();
const pipeline = new Pipeline(WORD2INDEX_URL, META_DATA_URL);


/* ---------------------- Syft Integration ---------------- */

/**
 * Prepare dummy train data generated by user.
 */
chrome.storage.local.get(['trainData'], data => {
    if(data && data.texts === undefined && data.labels === undefined) {
        let texts = [], labels = [];
        
        for(let i = 0 ; i < 5; i++) {
            texts.push('Privacy is good');
            labels.push(POSITIVE);
        }

        for(let i = 0; i < 5; i++) {
            texts.push("Tracking is bad");
            labels.push(NEGATIVE);
        }

        chrome.storage.local.set({'trainData': {
                                        'texts': texts, 
                                        'labels': labels
                                    }
                                });
        console.log('Created dummy train data');
    }
});


/**
 * Listen for messages from popup.
 */
chrome.runtime.onMessage.addListener( async (request, sender, sendResponse) => {
    if (request && request.action ==  'START_FL') {
        console.log('Request to start Federated Learning', request);
        startFL("ws://localhost:5001", "Cryptly", request.version);
        // Some testing.
        // chrome.storage.local.get(['trainData'], async (data) => {

        //     let rawTexts = data.trainData.texts;
        //     let rawLabels = data.trainData.labels;
            
        //     console.log("rawTexts", rawTexts);
        //     console.log("rawLabels", rawLabels);
            
        //     let processedTexts = []
        //     for(let i = 0; i < rawTexts.length; i++) {
        //         let curProText = await pipeline.process(rawTexts[i])
        //         processedTexts.push(...curProText);
        //     }

        //     console.log('processedTexts', processedTexts);
            
        //     try {
        //         console.log("const texts = tf.tensor2d(processedTexts);")
        //         const texts = tf.tensor(processedTexts);
        //         console.log(texts);
        //     }catch (err) {
        //         console.log(err);
        //     }

        //     try {
        //         console.log("const texts = tf.tensor2d(processedTexts, [rawTexts.length, pipeline.meta.max_length]);");
        //         const texts = tf.tensor2d(processedTexts, [rawTexts.length, pipeline.meta.max_length]);
        //         console.log(texts);
        //     } catch (err) {
        //         console.log(err);
        //     }      
            
            
        //     let processedTexts2 = []
        //     for(let i = 0; i < rawTexts.length; i++) {
        //         let curProText = await pipeline.process(rawTexts[i])
        //         processedTexts2.push(curProText);
        //     }

        //     try {
        //         console.log("const texts = tf.tensor(processedTexts);");
        //         const texts = tf.tensor(processedTexts2);
        //         console.log(texts);
        //     } catch (err) {
        //         console.log(err);
        //     }

        // });    
    }         
});


const startFL = async (url, modelName, modelVersion) => {
    const worker = new Syft({ url, verbose: true});
    const job = await worker.newJob({ modelName, modelVersion });

    job.start();

    // Writing a callabck hell
    job.on('accepted', ({ model, clientConfig }) => {
        console.log('Accepted into cycle!');

        // Load user's data
        chrome.storage.local.get(['trainData'], async (data) => {
            
            console.log("Loaded dummy train Data");

            let rawTexts = data.trainData.texts;
            let rawLabels = data.trainData.labels;
            console.log(rawTexts);
            
            let processedTexts = []
            for(let i = 0; i < rawTexts.length; i++) {
                processedTexts.push(await pipeline.process(rawTexts[i]));
            }

            const texts = tf.tensor(processedTexts);
            const labels = tf.tensor(rawLabels);

            console.log('Texts'); texts.print();
            console.log('labels'); labels.print();
            
            // todo: Shuffling the data

            // prepare train parameters, send by the server
            const batchSize = clientConfig.batch_size;
            const lr = clientConfig.lr;
            const numBatches = Math.ceil(rawTexts.length / batchSize);

            // Calculate total number of model updates
            // in case none of these options specified, we fallback to one loop
            // though all batches.
            const maxEpochs = clientConfig.max_epochs || 1;
            const maxUpdates = clientConfig.max_updates || maxEpochs * numBatches;
            const numUpdates = Math.min(maxUpdates, maxEpochs * numBatches);
            
            console.log({
                'batchSize': batchSize,
                'lr': lr,
                'numBatches': numBatches,
                'maxEpochs': maxEpochs,
                'maxUpdates': maxUpdates,
                'numUpdates': numUpdates
            });
            
            // Copy model to train it.
            let modelParams = [];
            for (let param of model.params) {
                modelParams.push(param.clone());
            }            
            
            // Main training loop.
            // for (let update = 0, batch = 0, epoch = 0; update < numUpdates; update++) {
            //     // todo: extract shuffled indices
            //     // Slice a batch.
            //     // const chunkSize = Math.min(batchSize, texts.shape[0] - batch * batchSize);
            //     // const startIndex = batch * batchSize;
            //     // const endIndex = startIndex + chunkSize;

            //     // const textBatch = tf.tensor2d({'values': texts.slice(startIndex, endIndex), 
            //     //                                 'dtype': 'int32'
            //     //                             });
            //     // const labelBatch = tf.tensor2d({'values': labels.slice(startIndex, endIndex),
            //     //                                  'dtype': 'int32'
            //     //                             });
                
            //     // Benefits of using plan -> 1. Let's us update the entire model architecture
            //     //                            2. Is very robust ?
                
            //     // cons: -> Right now it might throw a lot of errors.

            //     // Execute the plan and get updated model params back.
            //     // let [loss, acc, ...updatedModelParams] = await job.plans[
            //     //     'training_plan'
            //     // ].execute(
            //     //     job.worker,
            //     //     textBatch,
            //     //     labelBatch,
            //     //     chunkSize,
            //     //     lr,
            //     //     ...modelParams
            //     // );
        
            //     // Use updated model params in the next cycle.
            //     for (let i = 0; i < modelParams.length; i++) {
            //         modelParams[i].dispose();
            //         modelParams[i] = updatedModelParams[i];
            //     }
        
            //     batch++;
        
            //     // Check if we're out of batches (end of epoch).
            //     if (batch === numBatches) {
            //         batch = 0;
            //         epoch++;
            //     }
        
            //     // Free GPU memory. ? How is it GPU ? Is it cause tensorflow.js runs on WEBGL ?
            //     acc.dispose();
            //     loss.dispose();
            //     textBatch.dispose();
            //     labelBatch.dispose();
            // }
        
            // Clear local Storage ?

            // Save model for loal inference
            

            // TODO protocol execution
            // job.protocols['secure_aggregation'].execute();
        
            // Calc model diff.
            // const modelDiff = await model.createSerializedDiff(modelParams);
        
            // Report diff.
            // await job.report(modelDiff);
            console.log('Cycle is done!');
        
        });

    });

    job.on('rejected', ({ timeout }) => {
        // Handle the job rejection.
        if (timeout) {
          const msUntilRetry = timeout * 1000;
          // Try to join the job again in "msUntilRetry" milliseconds
          console.log(`Rejected from cycle, retry in ${timeout}`);
          setTimeout(job.start.bind(job), msUntilRetry);
        } else {
          console.log(
            `Rejected from cycle with no timeout, assuming Model training is complete.`
          );
        }
    });
    
    job.on('error', err => {
        console.log(`Error: ${err.message}`);
    });

};