import 'babel-polyfill';
import * as tf from '@tensorflow/tfjs';
import { Pipeline } from './pipeline';
import { Syft } from '@openmined/syft.js';
import { oneHot, losses } from '@tensorflow/tfjs';

const FIVE_SECONDS_IN_MS = 5000;

// Make sure this is consistent with the Creat Plan.ipynb
const NEGATIVE = 0;
const POSITIVE = 1;
// const NEUTRAL = 2;

/**
 * Check if blocked_urls exist in storage, if not
 * then initialize with an empty array.
 */
chrome.storage.sync.get(['blocked_urls'],data => {
    if(data.blocked_urls === undefined) {        
        chrome.storage.sync.set({'blocked_urls': []}, () => {
            console.log("Initialized blocked urls array");
        });
    }
});

/**
 * Add listener to recieve requests from tabs, and pass the text to 
 * pipeline and then to model for inference. Returns a response
 * to the tab with the prediction.
 * 
 * @param: message: { 
 *              action: Type of inference task,
 *              textAreaId: ID of the text Area on the page,
 *              text: Text which needs to be analyzed
 *          }
*/
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
    console.log('Recieved request', request);
    if (request && request.action && request.textAreaId && request.text) {
        
        if (request.action ==  'TEXT_SENTIMENT') {
        
            pipeline.process(request.text)
            .then(indices => sentimentClassifier.analyzeText(indices))
            .then(prediction => {
                chrome.tabs.sendMessage(sender.tab.id, {
                    action: 'TEXT_SENTIMENT_CLASSIFIED',
                    prediction: {'POSITIVE': prediction, 'NEGATIVE': 1 - prediction},
                    textAreaId: request.textAreaId
                });
            })
            .catch(err => console.log(err)); 
        }      
    }
});

const ROOT_URL = 'http://localhost:5000/yelp_polarity_review/'
const MODEL_URL = ROOT_URL + 'model.json'
const WORD2INDEX_URL = ROOT_URL + 'word2index.json'
const META_DATA_URL = ROOT_URL + 'meta.json'

class SentimentClassifier {
    constructor() {
        this.loadModel();
    }

    /**
     * Loads sentimentClassifier from URL and keeps a reference to it in the object.
     */
    async loadModel() {
        console.log('Loading model...');
        try {
            this.model = await tf.loadLayersModel(MODEL_URL);
            console.log("Model loaded."); this.model.summary();            
        } catch (e){
            console.error(`Unable to load model from URL: ${MODEL_URL}`);
            console.error(e);
        }
    }

    
    /**
     * Triggers the model to make a prediction on the text provided.
     *
     * @param {string} indices the indices output by Pipeline
     * @param {string} textAreaId Id of the textArea which has text
     * @param {*} tabId tab Id   
     */
    async analyzeText(indices) {
        
        if (!this.model) {
            console.log('Waiting for model to load....');
            setTimeout(() => { this.analyzeText(text) }, FIVE_SECONDS_IN_MS); // try after 5 second
            return;
        }

        try {

            const inputTensor = tf.tensor2d(indices, [1, indices.length], 'int32');
            inputTensor.print();

            // returns a tensor which is converte to array asynchronously
            let prediction = await this.model.predict(inputTensor).data();
            console.log("value ",prediction[0]);
            return prediction[0];
            
        } catch (err) {
            console.log("Error", err);
        }
    }
}

const sentimentClassifier = new SentimentClassifier();
const pipeline = new Pipeline(WORD2INDEX_URL, META_DATA_URL);


/* ---------------------- Syft Integration ---------------- */

/**
 * Prepare dummy train data generated by user.
 */
chrome.storage.local.get(['trainData'], data => {
    if(data && data.texts === undefined && data.labels === undefined) {
        let texts = [], labels = [];
        
        for(let i = 0 ; i < 5; i++) {
            texts.push('Privacy is good');
            labels.push(POSITIVE);
        }

        for(let i = 0; i < 5; i++) {
            texts.push("Tracking is bad");
            labels.push(NEGATIVE);
        }

        chrome.storage.local.set({'trainData': {
                                        'texts': texts, 
                                        'labels': labels
                                    }
                                });
        console.log('Created dummy train data');
    }
});


/**
 * Listen for messages from popup.
 */
chrome.runtime.onMessage.addListener( async (request, sender, sendResponse) => {
    if (request && request.action ==  'START_FL') {
        console.log('Request to start Federated Learning', request);
        startFL("ws://localhost:5001", "Cryptly", request.version);
        // Some testing.
        // chrome.storage.local.get(['trainData'], async (data) => {
        //     let rawTexts = data.trainData.texts;
        //     let rawLabels = data.trainData.labels;
        //     console.log("rawTexts", rawTexts);
        //     console.log("rawLabels", rawLabels);
            
        //     let processedTexts = []
        //     for(let i = 0; i < rawTexts.length; i++) {
        //         let curProText = await pipeline.process(rawTexts[i])
        //         processedTexts.push(...curProText);
        //     }

        //     console.log('processedTexts', processedTexts);
            
        //     try {
        //         console.log("const texts = tf.tensor2d(processedTexts);")
        //         const texts = tf.tensor(processedTexts);
        //         console.log(texts);
        //     }catch (err) {
        //         console.log(err);
        //     }

        //     try {
        //         console.log("const texts = tf.tensor2d(processedTexts, [rawTexts.length, pipeline.meta.max_length]);");
        //         const texts = tf.tensor2d(processedTexts, [rawTexts.length, pipeline.meta.max_length]);
        //         console.log(texts);
        //     } catch (err) {
        //         console.log(err);
        //     }      
            
            
        //     let processedTexts2 = []
        //     for(let i = 0; i < rawTexts.length; i++) {
        //         let curProText = await pipeline.process(rawTexts[i])
        //         processedTexts2.push(curProText);
        //     }
        //     try {
        //         console.log("const texts = tf.tensor(processedTexts);");
        //         const texts = tf.tensor(processedTexts2);
        //         console.log(texts);
        //     } catch (err) {
        //         console.log(err);
        //     }
        // });    
    }         
});


const startFL = async (url, modelName, modelVersion) => {
    const worker = new Syft({ url, verbose: true});
    const job = await worker.newJob({ modelName, modelVersion });

    job.start();

    // TODO: Replace callback functions with promises and async/await.
    job.on('accepted', ({ model, clientConfig }) => {
        console.log('Accepted into cycle!');

        // Load user's data
        chrome.storage.local.get(['trainData'], async (data) => {
            
            console.log("Loaded dummy train Data");

            let rawTexts = data.trainData.texts;
            let rawLabels = data.trainData.labels;
            
            let processedTexts = []
            for(let i = 0; i < rawTexts.length; i++) {
                processedTexts.push(await pipeline.process(rawTexts[i]));
            }
            
            let oneHotLabels = []
            for(let i = 0; i < rawLabels.length; i++) {
                oneHotLabels[i] = [0, 0];
                oneHotLabels[i][rawLabels[i]] = 1;
            }

            const texts = tf.tensor(processedTexts);
            const labels = tf.tensor(oneHotLabels);

            console.log('Local training data'); texts.print();
            console.log('Local training labels '); labels.print();
            
            // todo: Shuffling the data

            // prepare train parameters, send by the server
            const batchSize = clientConfig.batch_size;
            const lr = clientConfig.lr;
            const numBatches = Math.ceil(rawTexts.length / batchSize);

            // Calculate total number of model updates
            // in case none of these options specified, we fallback to one loop
            // though all batches.
            const maxEpochs = clientConfig.max_epochs || 1;
            const maxUpdates = clientConfig.max_updates || maxEpochs * numBatches;
            const numUpdates = Math.min(maxUpdates, maxEpochs * numBatches);
            
            console.log({
                'batchSize': batchSize,
                'lr': lr,
                'numBatches': numBatches,
                'maxEpochs': maxEpochs,
                'maxUpdates': maxUpdates,
                'numUpdates': numUpdates
            });
            
            // Embedding layer
            const embeddingLayer = sentimentClassifier.model.getLayer(name='embedding_1');
            embeddingLayer.trainable = false;   // Non-trainable
            
            // Flatten layer
            const flattenLayer = sentimentClassifier.model.getLayer(name='flatten_1');

            // Copy model to train it.
            let modelParams = [];
            for (let param of model.params) {
                modelParams.push(param.clone());
            }            
            
            // Main training loop.
            for (let update = 0, batch = 0, epoch = 0; update < numUpdates; update++) {
                // todo: extract shuffled indices
                
                // Slice a batch.
                const chunkSize = Math.min(batchSize, texts.shape[0] - batch * batchSize);
                const startIndex = batch * batchSize;
                // const endIndex = startIndex + chunkSize;

                const textBatch = texts.slice(startIndex, chunkSize);
                const labelBatch = labels.slice(startIndex, chunkSize);
                
                // Map word indices to word embeddings
                const embeddingBatch = flattenLayer.apply(embeddingLayer.apply(textBatch));
                // console.log("Flattened Word Embeddings"); embeddingBatch.print();

                // // Execute the plan and get updated model params back.
                let [loss, acc, ...updatedModelParams] = await job.plans[
                    'training_plan'
                ].execute(
                    job.worker,
                    embeddingBatch,
                    labelBatch,
                    chunkSize,
                    lr,
                    ...modelParams
                );

                console.log({
                    loss: await loss.array(),
                    acc: await acc.array(),
                });
                
                // Use updated model params in the next cycle.
                for (let i = 0; i < modelParams.length; i++) {
                    modelParams[i].dispose();
                    modelParams[i] = updatedModelParams[i];
                }
        
                batch++;
        
                // Check if we're out of batches (end of epoch).
                if (batch === numBatches) {
                    batch = 0;
                    epoch++;
                }
        
                // Free GPU memory. ? How is it GPU ? Is it cause tensorflow.js runs on WEBGL ?
                acc.dispose();
                loss.dispose();
                embeddingBatch.dispose();
                textBatch.dispose();
                labelBatch.dispose();
            }
        
            // Free GPU memor
            texts.dispose();
            labels.dispose();

            // TODO: Remove training data from local data as well ?
            // TODO: Save model for loal inference
            
            // TODO; protocol execution
            // job.protocols['secure_aggregation'].execute();
        
            // Calc model diff.
            const modelDiff = await model.createSerializedDiff(modelParams);
        
            // Report diff.
            await job.report(modelDiff);
            console.log('Cycle is done!');
        });

    });

    job.on('rejected', ({ timeout }) => {
        // Handle the job rejection.
        if (timeout) {
          const msUntilRetry = timeout * 1000;
          // Try to join the job again in "msUntilRetry" milliseconds
          console.log(`Rejected from cycle, retry in ${timeout}`);
          setTimeout(job.start.bind(job), msUntilRetry);
        } else {
          console.log(
            `Rejected from cycle with no timeout, assuming Model training is complete.`
          );
        }
    });
    
    job.on('error', err => {
        console.log(`Error: ${err.message}`);
    });

};
